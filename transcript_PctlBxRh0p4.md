# YouTube Transcript

- URL: https://www.youtube.com/watch?v=PctlBxRh0p4
- Video ID: PctlBxRh0p4

---

Two days ago, I attended what felt to me
like one of the most impactful
scientific meetings I have ever been to
and I felt compelled to share this with
you today. I do have some notes. I have
quite a few bullets that I want to get
to today. So, if you'll bear with me, I
think this will be worth it because this
meeting is transforming the way I think
about the future of science. And I
really do think you are going to value
hearing about this. So this is a special
and inaugural solo episode of the Cool
Boards podcast. Let's get into it. So
first up, let me just set this up with a
little bit of context here. So this was
a meeting about the impact of artificial
intelligence on science and it was held
at the Institute of Advanced Study. It
was an internal meeting. It wasn't
supposed to be like an open call that
people were invited to. And so in that
sense, my attendance was kind of
accidental. It was fortuitous. I
happened to be visiting Princeton on the
Tuesday to give the astronomy cloum
which I gave and had many great
interactions with faculty and students
that day. But then the following day
they often pack you off two miles down
the road to visit the Institute advanced
study which is its own separate
institute filled with many other great
astrophysicists amongst many other
academics as well. So, I was attending
the IAS that day, Institute of Advanced
Study, and there was this coffee meeting
at 11:00 a.m. led by one of the senior
astrophysicist faculty at the institute.
I won't say who it was to protect their
identity because, you know, there's a
lot to be shared here, but suffice to
say, this is somebody of enormous
scientific stature in the field who many
of us look up to, and he led this
meeting that was
It was fairly shocking actually to
attend it. I think a lot of what I'm
about to tell you was somewhat
internalized, at least for me
personally. I was well aware of the
impact of AI on my field. I had my own
thoughts about how it was benefiting me
um and the risks that it might have for
the future. But it was what what was so
shocking about this meeting is that all
of this was said out loud. This wasn't
just, you know, the voices in my own
head. Everybody was saying the same
thing. Everyone was saying the same.
We're all on the same chorus, right? And
that and that was what really struck me
was that these are people I hold in the
highest esteem and they are coming to
similar conclusions as myself. Now the
the is setting is important to this
story, right? Because the institute
advanced study is one of the most elite
intellectual institutes on planet earth.
This is where Einstein worked, right?
This is where Oppenheimer worked. If
you've seen the movie Oppenheim, the
scene shot there. This is the home of so
many generations of brilliant physicists
and many of the academics as well. Uh
just when I was walking down the
corridor, I actually passed Ed Whitten
who you know is is considered not only
one of the fathers of string theory but
one of the most brilliant living minds.
I mean one of the smartest human beings
on planet earth. And that's these are
the corridors that you are sharing at
the IAS. So I think that context is
important because I think it gives a lot
of credence to what I'm about to tell
you. Right? These these are big voices.
This is not Silicon Valley tech bros.
These are not billionaires. This is not
hype. These are people who have built
their career on analytic thinking,
technical work, mathematical
development, abstract thought. These are
truly thought leaders here. Okay. So
with that context said the meeting
itself what was it really about? So it
was led primarily by this one senior
faculty who spent most of the time about
maybe 30 40 minutes discussing sort of
agentic AI especially. So things like
Claude, uh, Kurszer, there was a little
bit of GBT discussion in there as well.
There was especially focus on those
models and how he was using it in his
own research and beyond in his own life.
He was, you know, organizing travel and
stuff like that and emails using these
systems, but primarily it was about
research. And he showed many examples of
research projects being achieved with
just a few prompts. and
how these systems were so powerful that
they were delivering very very
impressive results at the cutting edge
of what we would expect for a scientific
paper. So that was you know the first
sort of 30 minutes or so showing that
off and then after that there was also a
historian present. He was actually on
Zoom the historian but the historian uh
gave some comments towards the end of
the meeting and then it was opened up. I
mean there was just there was just some
discussion happening during the meeting
as well some hands coming up but then
there was kind of an open forum of
discussion at the end in which I
certainly participated as well. Okay.
So, the first big shocker, well, I don't
think it is a shocker. Like, I think I'd
internalized this already, but to hear
it said out loud
was was jarring. And this was that AI
models had now achieved complete coding
supremacy over humans software
development. And this is not small talk.
I mean, remember the audience who are in
this room, remember the people who are
here. Astrophysicists I mean they're not
by profession software developers per se
but they do an a lot of them do an
enormous amount of software development.
I mean you're you're thinking about
these large cosmological simulations
like Enzo, illustrous uh gadget
hydronamic simulations with adaptive
mesh grids and a lot of modern
especially modeling work in astrophysics
is heavily based in computers about how
to do these extremely sophisticated
software developments to simulate the
universe itself inside a machine. So you
know don't take that lightly. These
these are not these are not amateur
coders. These are some really very very
talented coders and there was not a
single hand that objected to this. There
was widespread
concession that AI was not just better
than humans, it had complete supremacy
and even the phrase order of magnitude
superior was being used in the room.
That was
I mean I it's certainly better than my
coding skills. I'm a decent coder but
not a great coder. And you know I just
assume oh you know probably there's
people out there who are much better
than me that aren't threatened by this.
But to hear that even those guys have
have just given up on this and that
they've they've conceded that was a big
deal. And to take this just a step
further it wasn't purely isolated to
coding supremacy. There was also a
concession that analytic reasoning,
problem solving, mathematics,
those skills were also comparable to the
level of ability of the current AI
systems at least. And perhaps even there
they also have some level of maybe not
quite supremacy but advantage, superior
ability at this point to to the people
in that room which remember
I I challenge you to find a room with a
high average IQ, right? This this really
was an astounding group of individuals
that attend. So um that was also quite
startling. Now I think there was an
admission that many of the earlier AI
models TGBT 3.54 etc things like this
those earlier models they certainly
often did hallucinate mathematical
solutions and that is still happening to
some degree but that is becoming less
and less and less frequent and the
results are becoming more and more
trustworthy. So we we still certainly
need to check the results. Everyone is
checking the results. We'll I'll come to
that point later. But the raw power of
these things when it gets it right is is
extremely impressive. I know my own work
uh I often do symbolic manipulation
solving differential equations,
integrals etc. with uh wolf from
products. So especially Mathematica um
but you can also use Wolf from Alpha
online um by Steven Wolfrram who of
course is a brilliant mathematician who
developed that software and I think
there was an idea that wolffram would be
eventually integrated into some of these
models like chat GBT I think it still
probably might happen at some point
maybe just as a cross check but I have
been astonished just as a personal
anecdote that in a recent uh set of
derivations I was working on Mathematica
could not solve a whole which isn't
unusual often you come across integral
it can't solve, but chat GBT could.
This was with 5.2 and I was blown away
by that. And not only could it solve
these integrals, and of course I did
check them, they were correct with
numerical evaluations. Um, it provided
the entire derivation which Mathematica
does not do. It showed all the
substitutions, the rearrange
arrangements, how it was actually
approaching this problem. And that was
uh obviously far more valuable than the
outcome the the output that you normally
get from Mathematica. So that struck me.
The lead faculty who was leading this
discussion said this. I kind of had to
write this down. Um
that these models in a very broad sense
of the word intellectually can already
do something like 90% of the things that
he can do. 90%. Now, he wasn't sure
about that number. He said it could be
60%, it could be 99%, but it was clear
in his mind that it was a majority
and it was only going to grow with
obviously future versions of these
models and it had you know broad
scientific ability with just a few
prompts. So
that was also startling to hit. This is
not just coding supremacy. This is much
more than that. Okay. Okay. So the next
point I want to raise was and this might
be unique to the to the senior faculty
who led this discussion. I'm not sure.
Although a few hands did agree with what
he was doing that he had totally
surrendered control of his digital life
essentially to agentic AI. So he'd
handed over his emails, his file system,
full access to his computer calendars.
He'd given pseudo control, super user
control as we'd say in Unix to his
personal machines and servers and disk
space, all of that.
And yeah, around a third of the room
when there was a a hand raised, ask for
hands raised said that they had also
installed these um agentic AI systems
doing this kind of stuff. So I certainly
haven't done that. I actually haven't
used any agentic AI systems. I've never
used Claude or Kurszer, which were the
two main uh systems people were talking
about here. I've just really been
interacting via a web portal with with
GPT whenever I've sort of needed these
systems. Um and that that made me feel
like I was way behind the curve actually
to some degree. But also people said,
you know, some of the people in the room
said, "Aren't you uncomfortable about
that? Like have you read the privacy
details of these contracts that you're
handing over your entire life to these
machines?" And what was startling is
that he just said, "I don't care.
I don't care.
The advantage that it affords me is so
great, is so outsized
that it's irrelevant to me that I'm
losing all these privacy controls." Now
you you many of us will probably
disagree with that I'm sure but that is
not that is not an extreme view because
people agreed with him in that room and
that was uh that was also startling for
me to hear that we are moving into an
era where not not just your average um
person using these machines but these
brilliant brilliant uh minds these
intellectual powerhouses are handing
over as much intellectual labor as they
possibly can to these machines. Along
the same lines, another startling
revelation here and this wasn't an
institutional endorsement, right? There
was no formal representative from the IS
administration here or Princeton I think
talking this was just among you know an
informal discussion kind of amongst
amongst academics but they were
embracing this and there was a sense of
how can we even accelerate the adoption
of these technologies. So they were
actually in the process, I mean one of
the purpose of this meeting was to form
a group where everyone could get into
these systems. They could learn how to
install them quickly. They could learn
how to set up skills and uh rules and
things like this and how to set up
prompts better, how to do prompt
engineering, all that stuff. So there
was a sense of let's not we're not
trying to resist this. We are trying to
get everybody who is interested at least
wi with a with a prod to do so clearly
by the sales pitch of all the numerous
successful examples the senior faculty
were showing to us that this should be
something you get on the train with this
this is not something that we encourage
you to resist. Embrace it. And so an
obvious counter to that and nobody
actually raised this point explicitly in
rebuttal to the lead discussion
but the the senior faculty leading
discussion acknowledged it is ethics
ethics and I know this bothers some of
you and we have that point later don't
we'll come to your your concerns about
ethics and other things with AI later
but it was acknowledged and
the the senior faculty head. You know,
I'm going to paraphrase a little bit
here, but but the tone was similar. You
know, I acknowledge, oh, there's all
these concerns, you know, billionaires
and it will replace jobs and uh power
consumption, climate change effects of
all this water and energy it's using and
those those concerns exist, but I don't
care. I don't care because we the
advantage is too great. And that was
startling. It was like ethics be damned,
right? And I'm not trying to throw this
scene factor under the bus because I
don't think it was an isolated view that
despite the fact there are many
reservations about using these models,
there was a sense that you have to do
this if you want to be competitive in
this environment.
And the advantages were so great that it
completely overwhelmed such concerns.
Now the next sort of concern that was
raised I think more during the
discussion the Q&A part of the the
session was about skill atrophy. Um now
this was this is my own analogy that I
thought of afterwards but you know a
good analogy is thinking about how we
use GPS systems navigation systems. If
you go back I don't know 15 20 years ago
or something before we all had these
little computers in our pockets that
could navigate you anywhere.
We we had I think a better sense of
direction. I certainly did. I kind of
had a 3D map in my mind of where I was.
But naturally, when you have these
awesome navigation machines, you just
defer to it. And it's it's easy. You can
drive and just kind of stop thinking
about where you're going and start
thinking about other, you know, physics
problems or whatever it is in your mind,
you know, or what you gonna have for
dinner tonight. You can kind of tune out
a little bit and concentrate less and
it's easier. Not everyone does that. I
know some people some of you probably
like are fighting that I'm not going to
use GPS. I'm going to I want to keep
those map skills uh fresh. But in the
same sense
if we defer certainly software
development that's already gone. uh are
we going to atrophy on our coding
skills? And clearly what's happening
already right now and maybe around the
corner is mathematical skills,
analytic skills, but even more general
than that is just problem solving.
Because with these agentic AI systems,
you just guide it. You set it up. You
just you define the problem and maybe
you know, you go a little bit further
than that and modularize it and
compartmentalize it, things like that.
We'll talk about that, but primarily
it's the one that solves the problem.
And
that's worrying, right? Will will we
forget how to do these core skills as
scientists? That sounds very
inattractive, right? To lose all of our
skill sets as scientists because we've
spent years, decades training for that,
right? I did an undergraduate degree at
Cambridge and then I did a masters and
then I did a PhD and then I had training
as a posttock. So it seems a little bit
wild that that anyone would be willing
to let go of all of that investment. But
I think the the case that was being made
for using these was a sort of sense of
inevitability and I think a lot of us
feel this with these AR systems
that if you don't use these systems the
IAS doesn't use these systems princes
Colombia doesn't use you know whoever
these institutions these top
institutions or even other institutions
that if they don't use them they just
will not be competitive in what is
happening what is to come and the
avalanche of discovery that might be
around the corner. So the adoption was
somewhat unavoidable.
And you know I was just thinking about
in the drive back I was thinking this is
a bit like Adam and Eve picking the
forbidden fruit off the tree right and
the serpent is is these AI companies
saying like here why don't you grab this
apple off the tree and the problem is
once you grab that apple you've lost
innocence. There's no way back once you
do that, right? Because of the risk of
skill atrophy. Um that you you yes,
you're going to have this this allure of
of enormous productivity boost. And I do
I do worry about the fact we're kind of
we have little choice here but to do it
if we want to be competitive. And yet we
recognize and are cognizant of the
the many deep concerns that it would
have about our own intellectual
abilities. So another point I want to
raise here is about training of these
models and I really mean human training
not not the training sets that they that
these AI companies are using. There was
an acknowledgement that when you first
encounter one of these models you've
only used it once or twice before in
your life and maybe it was even a year
or two ago when these things were
outdated. It's usually a pretty
frustrating experience. Like you know,
you ask it to do something and it it
just crap comes out the other end,
right? Is not what you wanted at all and
it can be very frustrating. And
especially because it talks to you like
a human,
you tend to interact with it like a
human. And that includes getting
emotionally sometimes angry at it that
is frustrated that it's not doing what
you want to do. And a lot of people just
kind of chuck it out the window and say,
"Yeah, forget this. it's quicker for me
just to do this myself than to use this
stupid model. And the leaf factor even
acknowledged that he said he's he has
wasted he call it wasted. He call it
investment. But he has spent a long time
trial and erroring how to how to use
these systems correctly. It's it's not
like you just pick it out the box and
you're going to get incredible results.
So this is like developing AI fluency,
how to set up a problem, how to set up
your workflow, um setting up these
different skills and and and systems
that will that get the results that you
want.
And so that, you know, in a sense that
means that people who do invest the
time, the early adopters that are
getting into the systems right now will
arguably have a significant edge. But I
think that's important to acknowledge
that you shouldn't just expect to pick
it up and and run with it. And that is
kind of the reason why this group were
interested in setting up an accelerator
to train the scientists in that room who
were interested was voluntary who were
interested in engaging with these
products and how to get up to speed with
them. All right, this is a big one. This
is human oversight and this this was
peppered throughout the entire
discussion. There was no part of this
session that this was not being
frequently reminded although I've just
put it as one block here. Um when you
produce these results they're not always
reliable right we we we all know that
they hallucinate especially that is
getting better but even today they still
I've had many interactions over the last
week where there was stuff that came out
that was just wrong and you could just
see it's wrong. So you still need to
cross check, right? Humans do need to
cross check these results. That could be
looking at plots, that could be looking
at snippets of code, running them
internally, checking things work, um
discussing the results with AI,
interpreting them, uh thinking about
them for yourself uh as well. You know,
so that the brain is still the human
brain is still activated. We we are not
completely discharged from any
intellectual oversight here. But it is a
very different kind of intellectual
activity.
It's it's more like the exclusive role
of just being a mentor or an adviser or
a manager really in that sense rather
than um necessarily
working through the problem yourself.
Indeed, a lot of those cross checks he
conceded he was doing them between AI
models. So he'd run the same problem
through Kurszer, then he'd put it
through Claude, then he'd discuss
results in GPT. So there was he was
making sure they all kind of agreed with
each other as well. And that was
obviously a way to sort of speed up that
cross-checking process. Um, and there
was also an acknowledgement that between
Kurszer and AI, when you use Kurszer,
I've never used either of these by the
way, but what he presented was that when
you use Kurszer, you can actually see
the changes made to your code. So
actually do a diff as we call it. So you
can actually see like it highlights this
bit of code I'm editing here in this new
vision. Whereas with Claude the way he
described it and some of you might know
better than me was that it kind of sends
off all these sub agents that solved
different from parts of the problem. And
so there's a lot of stuff happening
under the hood that is less transparent.
And so you might be opposed to that. And
indeed when this faculty was first using
these systems he strongly favored cursor
because he could see what was going on.
But as his trust level has grown with
these models, he's finding that
transparency actually an annoyance at
this point and would rather just he's so
willing to trust these models at this
point because he's seen evidently how
how impressive they are, how reliable
they are that he he's now switching to
claw because it's just faster and it can
go further in his words to achieve some
of these project goals. You know, you
can just use fewer prompts and let it
kind of get on with its own thing. So
that was also interesting that human
oversight was there but if there are
ways to even dial that back to a minimum
that's that's already happening.
I mean extrapolate that trend. Who knows
where this human oversight thing will
even persist. All right cost. I'm sorry
this we have a lot we have a lot of
points here. I appreciate appreciate you
stick with it. Hopefully this is
interesting to you but cost was
something I was worried about. I
actually raised this point in the
meeting. So
the scene factor who's doing this said
you know GPT is what 20 $20 a month for
the plus Claude because all these
systems have their own price tags and he
was already spending hundreds of dollars
a month of his own personal money to
have these systems
and you know that's what the power users
are doing and so that that already
prices a lot of people out right
especially you might imagine early
career people on lower salaries than
someone at the IAS would be priced out
and even people at other institutions or
other parts of the country or other
parts of the world might not have the
same kind of financial resources to buy
a a membership a subscription to all of
these AI systems. So
that's already introducing kind of an
inequity to a certain degree. And
further, the real concern I have here,
or another concern I have here, is the
exploding cost. Now, this hasn't
happened yet, but I'm just sort of
analogizing by thinking about what's
happened with so many subscription
services that we've got used to in life.
Um, just like Netflix and and streaming
services. Uh, but also just even
downloading software, right? It's kind
of rare that you can buy software and
it's just a one price and done. They all
want subscriptions and those
subscription prices creep up and up and
up. And we could easily imagine a world
where these models are so cheap right
now. We all get addicted to them. Our
skills go through this atrophy process.
We forget how to do coding, analytic
thinking, problem solving because we we
become so dependent on these machines
that two or three years in
they they kind of pull the rug from
under us and suddenly they say, "Okay,
now you need to start paying several
thousand per month for the same thing."
And
we many people probably will because
that the field has now kind of
acclimatized to this level of
productivity. And so the expectation has
shifted. The over you know the overton
window in a sense has shifted about what
normal is. And so that kind of worries
me that at any moment we could be
susceptible to these AI companies just
completely revamping that. And I raised
that point and the senior factory again
just didn't really care about that. He
said that, you know, doesn't matter like
the the it is what it is now and so I'm
going to use it now whilst it's
available because the boost is just so
big. So it's often just come back to
that same point. And to follow on with
the costing issue, another point here is
how are these you know think think from
the other side when you're playing chess
you always think about what the other
players thinking. What are what are
these AI companies going to do for
financial return here? because we know
there are trillions of dollars that
investors have put into these companies,
right? So, this is startling, but I
looked this up. There's the current
amount of AI investment since some like
2014, 2015 or so in the top three or
four companies now is over five times
that which was invested in the entire
Apollo program.
Over five times inflation adjusted and
over 50 times that of the Manhattan
project.
I don't think there's ever been a human
endeavor in all of human history that
we've invested so much into a a
technology into a project like this.
Now, that's a lot of money and they have
to get that back, right? The investors
expect to get that back. I know many of
you might be investors. Many of you
probably know a lot more about this than
I do. And I I'm just sort of wondering
how they're going to do that. And yeah,
it could be price jacking. They could
just raise the prices. But another thing
that was discussed not in the meeting
itself, but over lunch. And by the way,
I should say that once this coffee
meeting happened, the whole day of
discussion was dominated by AI. After
this, my lunches, my coffees, the dinner
is all people could talk about was AI.
It was really a startling kick to
everybody to focus on this. But what
came up over lunch was that one way
these companies could try to recover
some of that investment and why the AI
companies are pivoting a little bit
towards scientific development. You
often hear, you know, Samman talk about
fusion power and drug discovery, things
like this. And the reason is because
they could try and claim some of that IP
and some of the patent shares, right? So
you could imagine there one day being
like a GPT pro or something or research
grade uh version where part of the terms
of service mean that you give up I don't
know 10% 20% half of the IP to open AI
or whoever it is whichever company
you're using and
if they're going to dig out of the hole
and again I'm not economics expert I
also want to be clear about that I'm not
have no expertise in this whatsoever but
if they're going to dig out of this hole
which sounds enormous, $2 trillion or
so,
they need to get a lot of money back.
And so this kind of promise, and these
these are really my own words, this
wasn't something discussed in the
meeting. This promise that we often hear
from these tech billionaires that fusion
power will come and we have been this
age of abundance and there'll be so many
great free drugs.
How can it be cheap and affordable if
they have to reclaim all of those
investments? Surely that means they're
going to have to charge a lot for those
drugs, a lot for fusion power. I don't
know. And if that did happen, you would
have to kind of question the whole point
of doing this in the first place, right?
So the cost thing is interesting. I want
to hear your comments in the below about
that. All right, so let's shift gears a
little bit to who wins and who loses in
this new AI world. Now, traditionally in
a subject, a very technical subject like
physics and astrophysics,
those who have an edge, have an
advantage often have superior technical
skills. It could be, you know, solving
differential equations, abstract math.
It could be uh coding ability, thinking
of, you know, developing advanced
simulations.
There's a lot of technical skills that
go into a successful astrophysicist. And
I guess what's interesting is a lot of
those skills, those advantages are
neutralized by these models. So
just the the people who will be
successful going forward will not be the
same type of person potentially as we've
seen for the last few centuries in
science. It will be a different breed, a
different type of intellect and skill
set that will thrive. In New York,
there's this institute called the Flat
Iron Institute and they have a a group
there called CCA, the Center for
Computational Astrophysics. And the in
that in that department which is a
private foundation private funded by
private foundation they have had a
strong emphasis on hiring software
developers because this is often one of
those skills which is underappreciated
in in the job market. So if you're an
astrophysicist and your whole stick is
developing software for years and years
and years it's been hard to get a
faculty job then let's say your your
stickick was solving equations because
for whatever reason it was just kind of
looked down down upon by some physics
like oh that's not real physics you're
not a real physicist you're just a
software developer and so the CCA had
this big program where like you know
what that doesn't make sense because so
much of astrophysics is advanced by
software development we need these
people we need to prop them up we need
to have them in our field.
But with coding supremacy,
what happens to that? Will those jobs
still exist?
I'm curious. I mean, maybe I should talk
to Dave Spurgle, who's the head of Flat
Iron, and here, and hear his thoughts
about this at some point because he's
been on the podcast before, but I'm I'm
really curious like what's going to
happen to those kind of software
oriented positions.
May maybe they'll just look different.
Maybe there'll be people who really just
manage software development, and it's
the agentic AIs that do all the software
development. I don't know. But it's
certainly going to be disruptive to some
of those assumptions about who gets high
and who doesn't. The other interesting
thing here is is wealth, right? So if
you have more money, if your institution
has more money and once if they do, if
these models eventually jack up the
price and you have to pay for units of
of reasoning, however they price that,
then obviously having access to a
massive endowment at Harvard or
Princeton or Columbia would be
advantageous for your institution. And
so it could become a little bit more
like oligarchic disruption of of
actually educational and research
institutions themselves. And then
finally, just the actual people who will
thrive. I mean, I've kind of alluded to
this already, but I think they're people
who
will be able to compartmentalize
problems, break them down into
individual steps because that's often
how you feed them, at least at the
moment, into these models. And it's it
requires a lot of patience for the
reasons we talked about. Like these
models can be frustrating. And if you
get emotionally worked up easily and you
think of it as like a human interaction
because it certainly portrays itself
like a human interaction the way it
talks to you, it's easier to get like
pissed off with it and spend half an
hour and each the leaf faculty can see
that he said he spent hours like all
caps screaming on the keyboard at these
models like why didn't you do the thing
I want you to do you stupid thing. I
think we've all had that interaction
with these models and that that is not a
good that is not a good trait to have to
make the most of these models. You just
have to sort of understand that it's not
a person and you need to you know
control your own emotional reactions in
return when you're interacting with
them. So managerial skills, patience,
modularization, I think all of those
will be maybe what the next generation
of super scientists actually looks like.
So, who loses out of this? Now, we
already said that maybe people with
technical brilliance inherent, you know,
they just have a ridiculous ability to
solve equations. They're not going to be
advantaged. I don't if they're if they
lose, but they're not advantaged by the
system. The people who lose, I think
we're already kind of seeing this in
industry, right? So we have industry
internships and junior positions
evaporating because a lot of companies
you know especially software
developments tech companies they
recognize that it's just way cheaper way
cheaper for them to hire these uh use
these models rather than hire someone to
to do this work. So certainly that could
I don't think it really has yet but that
could proliferate into academic training
into research training that we might not
have the same number of graduate
students being trained across the
country the same number of PhDs the same
number of undergraduate degrees even um
and you know some of the motivation for
this is that I'm not trying to defend
this I'm not trying to say like oh we
should stop having graduate students
that's not to be clear I that's not a
world I'm looking forward would too, but
I'm just going to sort of give you three
reasons why this might happen. Okay,
just to be clear before my foot gets in
the keyboard. Okay, so uh training
scientists takes time and effort, right?
For me as a mentor, for any faculty who
has to teach someone how to do research,
it's a big amount of training and time
investment for us. I don't think people
recognize that. I'm always a little bit
amazed at the end of a 5-year PhD, which
is the length of PhD here in in the US
typically. Um, at the end of the degree,
you have this like fully formed
scientist and you can truly collaborate,
chat with them like a peer. They they're
at the peak of their game, you know,
they're they're running, they're
sprinting.
And when you go back just five years and
that you get this idea of like treating
all grad students the same, but then the
first year grad student walks through
your door and you sit down and start to
talk science with them and their face
just goes blank and you realize they
don't know really anything. I mean they
know some stuff but they're just their
level is so far below the fifth year
graduate student level you have to
recalibrate your expectation. It's was a
little bit disheartening. being like,
"Oh, man. All right, we have to for the
hundth time I have to go back to the
basics again with someone." And that's a
cycle we go through and it's it's
actually, you know, there's pro there's
a very rewarding aspect to that when you
see their eyes light up and they get
something. It's great. I I always love
that feeling as a teacher. You know that
because why would I do this these
YouTube videos? But it is a it is a huge
time sync for for a faculty. You're
having like several hours of meeting
every week with these people to train
them up. And you might have, you know, a
group of five students like that. So you
can easily block up half of your time
just training students. And you don't
have to train these models. They they
just out of the tin work. And it was
pointed out in this meeting that you
know especially like a first year PhD
project we normally give them kind of
like a light project you know like
here's a little project that I I could
probably solve this project in a month
maybe maybe even less than that but um
I'm going to give it to you to solve in
a year and that will be that'll be
helpful for you to learn how research
really works and it will be an
interesting result at the end of the day
as well but these models
they can do the same thing in like a few
prompts.
So it it does somewhat
hit that that that benefit of getting
more research done cuz you know if I did
it myself sure I could have done it
myself but it would have spent me maybe
a month of effort or two weeks of effort
let's say to do that but now I can do it
in an afternoon or a morning and then
the calculus might look different. Uh so
that's one factor. Another factor is
just the cost. So it costs me to my
grants when I hire a graduate student
once you fold in, you know, the the
health insurance and the tuition, all
this kind of stuff, it's a board of
$100,000 a year. That's typical for many
of the top institutions across the
country. $100,000 a year, which we fund
from grants and and of course some of
you are supporting Kard's lab directly
to and some of that money does indeed go
to supporting the training of people,
which I'm very grateful for. But compare
that to the cost of, you know, $20 a
month. Uh that that that's that's an
ocean apart, right? So certainly the in
this environment we have right now of
enormous grant pressure with the current
administration like a lot of federal
grants are being cut.
You it it's deincentivizing. Those two
things together compound a little bit to
deincentivize. And I think the third one
is you might say, well, look, none of
that matters because it's it's about
morality. It's about ethics. You have a
responsibility
as an educator to train scientists.
That's that's what we do. That's part of
our job. And it's not about how much
research they produce or how much they
cost. It's about producing this this
scientist who will go out into society
and do great things. That's the real
product. And I I believe that. Um but
the the counterweight to that again I'm
not advocating for this but I can
imagine a counterweight to that being
this is folly because there will be no
human scientists in 5 years. So why are
you spending 5 years training a
scientist with this ethical goal of adv
you know producing a human scientist
when those beings will not even exist in
five years. This is this is futile. I'm
not advocating for that. To be clear
once again I could imagine that argument
being made. So this is all just to say
there are several factors
which particularly early career
scientists
are
have the risk have a greater risk a
greater vulnerability of being displaced
or affected in a negative way I think by
these technologies maybe. um it it's
something we worry about because we
really care deeply
about keeping this flame of of research
going to the next generation and these
are all things that are that are deeply
worrying. So along the same lines as
grad admissions, so graduate admissions.
So when we when we admit students to
start a PhD at Columbia or whatever
institute is, we get a lot of
applications. It's typically it actually
went down a bit, well quite a bit this
year, I think across the country because
of international students finding being
deincentivized so much to come to the US
this year. But normally it's of order of
sort of 400
uh applications at least at Colombia for
basically four or five positions. So
it's a vort of like 1% admission rate
which is wild, right? I mean there's
already something kind of problematic
there but that that's the current rate.
Now that's a difficult process, right?
is yes there'll be some junk
applications in there but by and large a
lot of them are serious applications and
that we often try and nail it down to a
long list of maybe like 30 or 40 people
and when you look at that long list
you're like wow the these are all great
people have so many skills but it takes
a lot of time for the faculty to do that
work and there's a lot of disagreement
amongst the faculty and the admissions
committee about what kind of person are
we looking for you know what kind of
skills should we be focusing on for the
next for the next generation of the next
cohort that we're going to have in this
class? So there's two factors there like
one um well well first I say the actual
admissions process itself the senior
faculty who's leading this discussion
conceded that they indeed use these AI
models uh not to do their grad
admissions but to assist. I it's unclear
to me to what degree that assistance was
happening, but he said it it was the
fastest and most accurate. He felt he
felt the best about the reliability of
those admissions he'd ever felt.
But that's interesting. I'm sure on the
other hand applicants are using GBT
especially international students. I'm
sure because the English advantage of
using these models would be obvious the
language skills would would be enormous
by boost by doing that. So certainly I
suspect applicants are using these
models. But interestingly on the other
side it's already happening.
That's interesting. I don't know what to
really comment about that but that's an
interesting point I want to share with
you. The second is what skills are we
really selecting for. So there there is
no strict agreement amongst different
institutions even different faculty
within the same institution about what
skills I want to select for. Some think
that we need to, you know, focus on
collaborative skills. That's like the
most important thing because science is
inherently collaborative. Others would
argue, no, it's about their technical
ability. It's about their ability to to
write these simulation codes. Like how
how much hydro simulations have they
done so far? Let's look at their track
record with that. Others say it doesn't
really matter. It's just about how many
research experiences they've had. How do
they present? How do they communicate?
Because communication is fundamental to
what's the point in being a great
scientist if you can't tell people about
what you've done. So there's there's a
there's a wide array of different skills
that we think about when we select grad
students. No one no one neuters any of
those. They all just have different
weights, right? So there'll be different
some people say they're all equally
valid. Other people will say that
technical skills are more important.
Other people say collaborative. So it's
just different.
And you do have to wonder because the
degree is five years at least. Some
people take a bit more than that. It's a
five-year degree and we are admitting
people with a certain set of criteria
which are largely traditional I think at
this point the same criteria we've more
or less been using for the past probably
100 years or so and you do have to
wonder whether the people we're
selecting
are truly the right people are are do
they have the right skill set to thrive
in an environment in 5 years time from
now
and I mean that that's a recurring
problem I I kids. I have two young
children. I have the same kind of issue.
I just don't know what to tell them to
do, what to focus on, what training to
to look at. And that's just another kind
of side of this coin is when we do
admissions, what skills do we select
for? And certainly I've been thinking a
lot recently like would I actually admit
a student anymore or not admit, but
because that's done by the admissions
committee, but would I agree to work
with a student who refused to use these
models completely, which some students
feel that way. like I I won't touch them
for various reasons. I'm sure you have
many objections yourselves you can think
of for that. But it's not obvious to me.
I would admit I'd agree to work with a
student like that. As hard as it is for
me to confess that because the because
of kind of what was said by that senior
faculty leading this discussion, the
advantages are so enormous. It's like
working with someone who refuses to use
the internet or refuses to code in in
any software language. It's it's like
you are putting like two giant hands
tied behind your back by doing that.
It's I I don't know if I can work with
someone who who refuses to use the
modern research tool set that becomes
like the standard way of doing science
and and that that's not quite true yet,
but it's clearly on the cusp of
happening. So
yeah, I'm still processing a lot of this
as you can tell. I don't have answers
for you, but I think this is interesting
to think about. Okay, last last page. I
promise you, we're on the last page now.
So, the other uh thing that might lose
here a little bit is collaborations.
Um, some of you might not realize this,
but when you do when you're a scientist,
you often need things from other people,
right? So, everyone has a finite skill
set. There's certain things I can and
can't do. uh that I might need like an
endbody dynamical code for instance and
speak to a dynamicist about that. And so
I could grab the code, I could use it,
but you know to like truly edit it in a
nuanced way to get it to do what you
want it to do to maybe do something
different to think about a new problem
you need really that expertise. Um just
as an example and there's many many
types of collaborations of course that
exist. But there is also a bit of
inertia for some people. For some people
it's totally elevating. This isn't
inertia at all depending on your
personality type. But you obviously have
to reach out to these people. That's
that's an obvious thing you have to do
to collaborate with someone. You have to
email them. You have to chat with them
at a conference. You have to set up, you
know, probably multi-hour Zoom sessions
with them to discuss the problem, look
at the results, reflect on it.
There's a time investment there. And
especially if it's quick and especially
if it's something really small like I
just need this one tiny little
calculation that you you're the guy for
then
those I worry might disappear right
because if I just need like a little
thing especially
then what's easier for me to send a
string of three or four emails without
respond keep going hey just re-uping
this I know you're busy but do you have
time to look at this problem
and they have to like read the whole
paper to the problem and it's just it's
it's a lot of time investment for both
parties for like this small increment.
You can see there that AI would be
preferable. And so yeah, I do wonder
whether the whether collaborations will
diminish. I guess we'll see. But you
know, you'll see it reflects in the
publication record if that's true. But I
wonder if collaborations will shrink to
kind of just the core people you really
need and eventually maybe even just
single authors, right? I actually often
write papers single author. Um but I
wonder if that might become more common.
It it's kind of unusual actually these
days for someone to try a single author
paper despite my peculiarity in that
sense. But I I question whether that
might become more common. Although I do
want to acknowledge that
some people do science for the
collaboration, right? Some people that
that is the point of it like they're
they love that social interaction and
engaging with other people directly in
the room. And for those people this this
is not maybe applicable but for many
others there's a lot of introverts in
science that that's all I'm saying right
and and for those small things
especially I I can imagine this being an
issue okay so last one people like me
tenure faculty and I'll talk about me
specifically in a minute but tenure
faculty I think everyone in the room
acknowledged this and especially in
private conversations which I had
throughout the day at the is afterwards
where this thing kept coming up over and
over again as I said that tenure faculty
will be fine if anyone's if there's
going to be any victims or job
displacement here. I mean eventually we
could get displaced but we're going to
be the last people on the boat right as
the Titanics going down with the
captains that go down with that ship as
the institutions themselves for tenure
tenure faly would go down with it of
course it's the very definition of
tenure you'd have you'd have to rip it
up to get rid of these people right so
they're probably fine probably you never
know but they're they're probably the
last ones to go and uh I that is that
that was interesting that th those
people were freely admitting that. But I
you know I don't pretend that my job is
infinitely safe because that's the whole
kind of point of this video
this this job could go and it actually
could be one of the first jobs to go. I
know by the way I said my last video was
also about I touched on AI a little bit
about how it could affect in a very
positive way I think the development of
telescopes. I am excited about that how
it could help and I I commented in there
and it generated a lot of comments back
that with a caveat that I'm not an
economist that I'm a little bit
skeptical it could displace all jobs
that's what I said all jobs and be clear
I mean all jobs scientists is not all
jobs I I do think we are we are
unusually vulnerable for displacement
you know if you're going to rank order
who's who's first on the cut on the
chopping block
people doing intellectual labor would
obviously be likely to be the most
vulnerable if you have a machine that
can do intellectual labor. Physical
labor, no. So obviously there's plenty
of jobs which are which are physical
labor and that's why I wouldn't say all
jobs
but also I'm not an economist so I I
don't know what I'm talking about in
that sense but I do think I do think
it's certainly with science I know what
I'm talking about and there is there is
a great worry as as we've all throughout
this video that the skill sets we have
are replicable in fact even superior by
some of these machines and just on this
point one more thing related to tenure
faculty is how we act as mentors and
trained students and
it's it's hard to find that balance. I
mentioned I wasn't sure about whether
I'd hire a student who didn't use AI or
not. I'm I'm still processing that. I'm
still thinking about that. There's
obviously a balance, right? Because you
don't want to hire someone who's just
brain dead because they just do
everything on they just type it into a
prompt and that's it and they never
really think. We we don't obviously want
people who who get that kind of forget
how to how to use their heads, but at
the same time there's clearly a balance
where you use it in a thoughtful way to
boost your productivity. And so that
balance
that's that's the that's the thing we're
looking for here. And I I do not know
how to find that balance, but I think it
is a very important point that we we
have to think about how to identify
that. Okay. So me me me me me me me me
me me me me me me me me me me me me me
me me me me me me me me me me me me me
me me me me me me me me me me me me me
me me me me me me me me me me me me me
me me me me me me me me me me me me me
me me me me me me me me me person
personally and AI. So yeah, I'll talk
about public backlash next. I want to
acknowledge that because well maybe I
just just contextualize it a little bit
there. I I do notice especially in the
last video whenever or if I use like an
AI thumbnail or something or an AI
visual in the video, there's a lot of
you get upset and you don't like it and
there's a lot I don't want any AI slop.
So let me talk about my use of AI. I
want to be totally transparent with you.
I don't want you to think that I've just
suddenly started using AI and this is
like a new thing. I have been using AI
for as almost as long as AI has existed.
So for over a decade I have been develop
I've not just been using AI I've
actually been developing AI models and
there's uh we've been using machine
learning and AI models deep neural
networks to predict the stability of
circum binary plants and predicting
missing plants in multiplanet systems
for instance for for over 10 years as I
said and I I helped write that code. So
I kind of got out of the game about
eight or nine years ago because it felt
like well I was still using the models
but I stopped developing the AI models
and it wasn't for an ethical reason. It
was just because I couldn't actually
keep up with the pace of the literature.
It kind of felt like either you go full
AI at this point in terms of your
intellectual interests or if you want to
remain broad like I did it's you're
probably not going to be competitive by
just dabbling in AI anymore. So that's
why I kind of took my foot out of it a
little bit but I still use the tools in
many many facets and always have. So
proof proof checking papers is a real
obvious one. Often we'll copy and paste
the latex source in and make it look for
both factual errors. Uh grammatical
checks, things like that. Vibe coding. I
mean, yeah, software development,
writing code. I I don't write all my
code actually with AI. That's not true.
Actually, I probably still write most of
my code myself, but um
I often use it to generate functions and
snippets of code, things like that, I'd
say. But I'd still say the majority of
the actual lines I type are probably my
own lines, but I I do use it a lot for
vibe coding. Debugging. I mean, yeah, I
I almost never debug code myself
anymore, I have to say. I pretty much
always get an error message and I just
copy and paste the snippet and the error
message and say, "What's wrong with
this?" It's just way faster. It's not
It's not why I became a scientist to
debug code, believe it or not. So,
that's just easier. Uh proposal and
report, not writing, but revisions.
Again, that kind of proofing process.
Um, and really like think about how can
I make this a compelling proposal. Think
about ways that this could really like,
you know, be be interesting to the
reviewer, like asking those kind of
questions. Uh, literature search, uh,
when you're trying to look for a paper,
you're trying to see, has anyone ever
done this before? It does, that's kind
of one of its worst things actually at
the moment is often hallucinate, um,
references, but you could, it's actually
pretty simple just to Google the
reference it told you and see that it's
hallucinated. So, it's not that hard to
check that. uh derivations
in in papers um like I said I've used
that quite a bit tar right as an example
tar so doing interdicciplinary stuff so
tar is a lot of physics and a lot of
that physics I did but there's also a
lot of material science in it I didn't
know any material science so I relied
heavily on um you know discovering like
what are the tensar strengths what kind
of materials exist um what's the
properties of graphine sheets what what
materials have the right albido
how many atom layer thick do I need of
this to avoid certain transparency
effects that kind of stuff I mean I can
do it but it's just way easier to to
consult with a material expert in that
case which was the model um and I want
to concede that I'm not a super user I'm
not in the same bracket as this guy
who's leading this discussion um and I
think my scientific strength has always
been being creative and I think that's
actually kind of supercharged by these
AI models so in that sense I'm enjoying
using the models because I think it
enables me to do more with the skill set
that I have. And then with videos,
because that's just research with
videos, yeah, I do use I want to be
honest with you, we use AI stuff quite a
bit. Um, we use a package called DX
Revive to clean audio to make it kind of
have this kind of studio quality sound.
Not always, but sometimes depend like if
it's rough audio, we'll often use it.
Uh, you know, we often take B-roll from
movies and things and sometimes we need
to strip out the background music to fit
with our own score. So, we use a package
called LA.ai.
If you know it, you know it. Um, rev.com
for transcripts. Uh, we use Topaz
sometimes for upscaling poor quality
clips. We use GBT for factchecking
scripts, video scripts, generating title
ideas, brainstorming ideas on the
titles, uh, thumbnails. Yeah, we don't
we don't we sometimes have you know we
know we have we used a couple of maybe
two or three times we've used AI models
to gen the entire visual like a gen AI
model for the thumbnail but it's kind of
unusual it's more common that I'll use
it for like removing a background or
something like smartly removing
background or even adding puffing a
padding a background something like that
but I do want to say that I have some
video some of my most popular videos are
these stories like uh journey to the end
of the universe outlasting the universe
things like this the first civilization.
In the first civilization, we
commissioned an artist to generate uh
some some images and she drew I think
five images for me and I had to really
like spin those five images to not
repeat them over and over again. So, I
kind of cropped them in different ways,
mirrored them, colorized them in
different ways. So, yeah, I was trying
to like really stretch that material.
And I didn't really want to do that
again. I have another narrative I want
to do, another narrative video. And I
was like, I don't want to do that again
because the quality just it did the
video did great, but I was just
personally thinking the quality is not
there like in terms of the visuals. But
with these AI models, I have been
chatting to my editor recently, Huhe,
and we've been saying like this enables
us to do stuff we couldn't normally do.
Like we could make these narratives that
we always thought were basically
impossible
without having an entire, you know,
production crew behind us. Uh so that
it's enabling in that sense. I don't
know but I'm a little bit hesitant to do
that that story now because the reaction
as we'll talk about in a moment has been
really strong against those against
those AI visuals. So um I want to
acknowledge that. So all this is just to
say I've been using a for a while for
over 10 years and this is this is not a
sharp transition of of cool words videos
or cools podcast or my research suddenly
adopting AI. It's just been been a
continuum in that sense but backlash
let's talk about backlash. So yeah,
there is a strong public reaction with
the common comments being AI slop and I
know something like AI is evil of that
kind. AI slop I obviously I totally
agree like I hate AI slop. I guess to me
it's just I define it differently. For
me AI slop would be like the whole video
is just spam. Like you've it's just
basically Wikipedia. It's just reading
out Wikipedia or it's, you know,
spinning like a an idea from Reddit or
something like just something that's all
over the internet. It's not novel. It's
not new. It's just recycling the same
old junk round and round. That to me is
what I think of as AI slop. You might
have your own definition. I'm interested
to hear it if true. And to me, maybe you
disagree. I I think the videos we do I
only interested in doing a video if I
feel like I have something totally
hopefully totally unique to say about a
topic. It is a unique insight you will
not find elsewhere. That is always my
goal and that's why I don't do a lot of
news videos cuz it's like what can you
say? Like here's the news that
everyone's reporting the same news. It's
it's it doesn't appeal to me. I always
want to offer something new and fresh.
So yes, we might sometimes use a visual
or something like that that's that's
come from AI, but yeah, that's why I I
would probably defend a little bit that
I don't think that our content is AI
slop by any stretch of the imagination.
Uh but I I I do want to hear what you
think about this. If we you know, if you
strongly feel like we just shouldn't use
it at all, that's that's interesting
data point to me and I definitely listen
and read your comments and it has a big
impact on what we do. So please take
that under adisement. And in terms of AI
being evil, um yeah, that's the other
one that that some people just feel like
ethically it's just totally wrong. And
you can certainly make that argument and
very sympathetic argument because of the
concerns we all might have about how it
changes society in potentially negative
ways. Um something like social media,
you could easily make an argument,
right? Social media has all these
negative effects. Weirdly, I never put
YouTube in that bucket. I was like
YouTube's not really a social media. it
it it's so much more than than what
Instagram or Tik Tok are delivering,
right? Because you got this amazing
educational stuff on here, which is kind
of unique. Um,
but well, there is a little bit of that
on those other platforms as well. But I
think YouTube really thrives in that
environment. So, you can certainly make
the argument that there are these
technologies that are that are bad and
we should monitor them. We should get
rid of them. But it's also true that
there've always been technological
developments that have have changed
jobs. I mean, that that's always true.
So
if you were around when Henry Ford was
inventing the car, right, the
automobile, and your your profession was
was looking after horses, I know, doing
their hooves or something, uh cleaning
them, looking after them, giving the hay
bells for them, whatever it is, then you
might be worried, rightfully so, that
your job is about to be displaced. But I
don't think society should pivot to
protect the jobs of the horse groomers.
That that's not progress. And so it it
might be that that some routine things
we do in video production do get
displaced from using Fiverr or you know
this gig economy stuff which we normally
do for some of this kind of stuff. Maybe
some of that will get displaced but it's
not obvious to me that that necessarily
makes it evil. But maybe I'm a little
bit biased here because we have been
using it and we've been impressed with
it. So yeah, this is I'm I'm really
trying to process this and think through
it and I do want to be thoughtful and
listen to both sides. So this is a great
place. I think a lot of people have
opinions about this. So please do let me
know your think your thoughts here. I
guess the one the one thing I did write
down I didn't say about the public
backlash issue and that the people in
that IAS room I think were largely
oblivious m at least much more oblivious
to public backlash than someone like me
a scioma I have my foot in both worlds
right I'm I'm talking to you guys who
are who are just normal folks out there
just interested in science and want to
engage with it but you have other lives
outside of science as well and then
there's these kind
elite academics who often don't do a lot
of science communication and that they
don't have much contact with the
reactions from the public in that sense.
So I do get the thought the idea from
from interacting with them that they
were a little bit oblivious to any
concerns about AI being a problem to
using their work. They they certainly
were not worried that if I write a paper
that was 99% generated by AI that the
public would in any way not like that.
that that was not a topic that anyone
voiced or cons maybe they did in the
back of their head but it was certainly
wasn't voiced in this discussion and I
think the reason is because there was no
fear that AI assisted science was
illegitimate because they acknowledged
that its skills were already comparable
if not superior to their own research
skills. So I think that's the core of it
as to why this just wasn't something
they were worried about. historical
significance. There was a historian in
the room and they explicitly framed this
as a transitional period for society and
yeah they said this is a historic moment
and they didn't mean that meeting
actually to me that meeting was a little
bit historic but I don't think they
meant that meeting I think they meant
that this moment in society for science
specifically is is one of those moments
that historians will look back at and be
interested in and therefore or he said
when he actually the the historian said
this there was a laugh there was a
giggle and a laugh in the room but they
said we should be documenting what's
happening right now the way and that's
actually what inspired me to make this
video that's why I'm doing this video to
document what it feels like in this
moment because this is very different
from how it felt a few years ago my last
uh real AI video was about you know chat
GBT 3.5 passing my passing my my 101
exams
And I'm sure there's going to be very
which feels like a world away. And I'm
sure in 5 years like this video I will
look back at and be like, "Wow, there's
so much I got wrong. There's so much we
we completely misjudged. It ended up
completely different. I don't know how
it would be different, but I'm sure I'd
be wrong in so many degrees." And that's
I think that's why it's interesting to
make this video to document it to
express this is not a lot of these views
are my own views but a lot of what I've
told you is is is a very common
sentiment that was being held by the
most elite top brains we have in
astrophysics and that is that is
important and relevant and then two two
final points I want to make
democratization of science and paper
tsunami what I've written down here So
yeah, science is normally done by people
like me, right? At least astrophysics is
normally certainly done by people like
me. And that's someone who has spent
three or four years in undergraduate
school, a master's degree, a you know
five years or in my case three and a
half years getting a PhD and then many
years of training and th those are the
people who normally do science and they
are in the ivory tower right we we we
are the product of this this filtering
like you take the cream off the top and
you do that again and again again you
just end up with this like super
ratified rarified air at the top and
that that are the people like me who do
science and that might change
because
I often get people many of you guys have
emailed me with ideas. I love it when I
get when I get your your ideas like what
about this idea? Have you tried um
changing tasks to do this or have you
tried looking for exomoons in this new
way? And those are always really cool. I
always have finite time, right? As you
can imagine, I actually have negative
time, not just finite time, of negative
time to pursue all these different
ideas. But you don't need me anymore.
May maybe maybe already, but if not very
soon, you do not need me anymore to
pursue your own interesting research
ideas. You can just consult with these
models for 20 bucks a month and do a
research paper yourself and it could
easily be comparable. I don't know.
That's going to be interesting to see.
Would the quality of those papers be
indistinguishable to some degree from
the quality of papers that someone like
me would produce with our two decades of
training behind us?
That's really interesting because it
could mean that science is done by just
anyone really.
And that that is a big changing of the
guard, right? So I mean I shouldn't
encourage that because my job is
supposed to be dependent on this unique
skill set. But I do I do think it's kind
of that that same point of technical
skills are being neutralized by it's
it's a great democratization of science
by everyone can participate now in a way
that's never really been possible before
at this scale. And you the consequence
of that is if everyone can do science
and lots and lots more people do end up
doing science
there will be a lot of science which is
great
but the downside of that is that means a
lot of papers. How how do we even read
that many papers because already every
day on archive which is the sort of the
repository of new papers in astronomy in
all sorts of fields there are already
often dozens of papers to read every day
and you just you don't have time to read
them. You have to like scan through the
titles and abstracts as fast as you can.
But we could end up with like an order
or two orders of magnitude more papers
with this democratization of science of
generate and not just more people doing
science but even scientists being super
users who can now do three or 4x papers
per year than they used to. So I don't
know what the answer to that is either
but there just could be this absolute
flood and tsunami wave of papers of new
research and how do we ingest that and
you can just say well AI use AI to
ingest that but that's not quite the
same thing I need to get in my brain to
be a scientist right it's not so
I don't know if AI can help with that
just yet unless you do Neuralink or
something so that raises a whole
question of like how are we going to
cope with this flood of of work that's
going to come and then finally Finally,
just to finish off, um this is just a
personal note here.
Why why do we even do science? Um what's
the point of it? Now, I know to many
people who are not scientists, and I've
said this on the channel before, so you
probably heard me and a lot of people,
you disagree with me. Seems to happen a
lot these days. A lot of people disagree
with me. But what I said was the reason
why I personally love science is that
I'm a participating actor in it, right?
I like finding things out. I love that
process of doing it. It's like being a
detective of going onto a crime scene
and trying to figure out what happened.
And it's like when you watch a crime
show, you don't want the spoiler. You
want you want to play along. You want
try to figure it out for you. It's not
much fun just to read the spoiler before
you even turn on the show of who the
killer was and then try and watch it.
And yeah, I I don't know um if I what
level of enjoyment I would have for a
world of science where humans and myself
were cut out. We're just not a part of
the discovery process anymore. Maybe we
like the fact checkers, but even that I
think you can imagine disappearing at
some point. Um and that that worries me.
I think like science is a bit like art
in that sense that you know sure you can
have AI generated art and it has its
place. It's useful. It's interesting. It
can to some degree beautify the world to
a great extent by having more art in the
world. But the art I'm always most
compelled about and if I was going to go
to a museum and see like the top art,
the best, what is the very best that's
producable, then I I'm interested in
like the human story, like what what
motivated that artist, what was going on
in their lives, what inspired them to
produce that. And science isn't quite
the same, but it is still a human
endeavor. It is an act of human beings
doing something for the sheer kind of
curiosity and love of doing it. I really
do believe that science is intrinsically
a human endeavor. And if we have these
AI models that deliver fusion that
deliver all these drugs that deliver new
theoretical physics breakthroughs, but
it's you know if if this is from a super
intelligence, these discoveries might be
incomprehensible to me and and to many
others. Maybe no human being will
understand how this fusion machine works
that it produces.
And there's obviously an economic
benefit to that. No doubt that that's
interesting and worthwhile. But it does
subtract somewhat from this
multi-millennial
period we've been in of humans being
driven to understand the world around us
to transition into a world where we just
don't. We just have no understanding of
the world around us. That frightens me a
little bit. I I don't know that I want
to live in a world where everything
around me is just magic.
Living in a fantasy world. I want to
live in a world of where I actually can
comprehend it. A comprehens
comprehensible world. So that is a
thought that that I kind of want to
finish on just thinking about is what is
even the point of replacing all human
scientists with machines.
I don't know. I don't know. Like I said,
there's a lot in this video. As you can
tell, this is very much a live, very
unlike our usual videos. Live,
unscripted, raw conversation. And I
think it's a conversation we have to
have. It's not a conversation I've seen
done in quite this way before on
YouTube. Hopefully, you'll agree this is
fairly different.
And like I said, the fact the real
kicker for me I want to leave you with
is that this is not this is not my own
personal
this is just all in my head that I'm
worried about AI. This is a conversation
that the most elite institutes in the
world are having emergency internal
meetings about. And the smartest people
you can name in the world, you can think
of are worried about this. And they see
this as a threat to their intellectual
supremacy and have even conceded
much of their ground to that already.
already.
That is uh that that makes me think that
some this is this is really happening.
This is really happening. This is not
just uh this is coming down the pipe
someday. Like we're in it. We're
swimming in it. So,
lots to say. I I'll try and do chapters.
I had 20 25ish points on here. I'll try
and do chapters for each of these points
so you can flick through, go to the
different ones. But yeah, please do let
me know your thoughts on any of this.
There's lots of points here. You could
you could offer your opinion. Throw
multiple comments. Tell me what you
think. And this is a conversation not
just with scientists but with the public
cuz you guys are funding everything we
do. And what is the point of science if
it's not for public benefit. So uh this
this is relevant to not just the future
of this channel, not just uh not just
not just even science itself, but
society itself and how it interacts with
these developments. So please do let me
know your thoughts and thank you so much
for your patience. If you made it to the
end of this video, I appreciate that.
Please do give us a little like and a
share. Uh that always helps. And as
always guys, stay thoughtful and stay
curious.
