# Figures and Tables

Figures are where incorrect code becomes persuasive. A plot can look professional while being completely wrong. This page covers how to make figures you can trust.

## The golden standard

Every figure in a paper should satisfy:

1. **One command regenerates it** - `make fig2` or `python scripts/make_fig_2.py`
2. **Clear source script** - anyone can find the code that made it
3. **Recorded inputs** - which data files, which parameters, which random seed
4. **Visible provenance** - the figure itself (or a companion file) says how it was made

If you cannot regenerate a figure from scratch, you cannot trust it.

## A practical file structure

```text
project/
  scripts/
    make_fig_1.py
    make_fig_2.py
    make_all_figures.py
  configs/
    fig_1.yaml
    fig_2.yaml
  results/
    figures/
      fig_1.png
      fig_1_provenance.json
      fig_2.png
      fig_2_provenance.json
  data/
    raw/
    processed/
```

Each figure script reads a config and writes both the figure and a provenance record.

## What to put in a config file

```yaml
# configs/fig_2.yaml
data_file: data/processed/experiment_results.csv
output_path: results/figures/fig_2.png
random_seed: 42
filter_condition: "quality > 0.8"
plot_style: paper
dpi: 300
figure_size: [8, 6]
```

This makes it trivial to regenerate figures and see exactly what parameters produced them.

## Provenance records

After generating a figure, write a small JSON file alongside it:

```json
{
  "generated_at": "2026-01-15T14:32:00Z",
  "script": "scripts/make_fig_2.py",
  "config": "configs/fig_2.yaml",
  "data_hash": "sha256:abc123...",
  "git_commit": "d6958af",
  "python_version": "3.11.5",
  "key_packages": {
    "matplotlib": "3.8.0",
    "pandas": "2.1.0"
  }
}
```

This answers the question "how was this figure made?" months later.

## Verification checks for figures

Before trusting any AI-generated figure code:

### Visual sanity checks
- Are axes labeled with units?
- Is the scale sensible? (Check min/max values)
- Are you plotting what you think you're plotting?
- Do colors/legends match what they claim to represent?

### Data sanity checks
- How many data points? (Print the count)
- What's the range? (Print min/max/mean)
- Any unexpected NaNs or infinities?
- Does a random subsample look reasonable?

### Reproducibility checks
- Run it twice with the same seed—identical output?
- Run it on a fresh clone—still works?
- Change one parameter—output changes as expected?

## Common figure mistakes AI makes

### Wrong column plotted
The agent picks `column_a` when you meant `column_b`. Always verify which data is actually being plotted.

**Fix:** Add a print statement showing the first few values of what's being plotted.

### Silent filtering
The agent adds a filter that drops half your data without telling you.

**Fix:** Print data shape before and after any filtering step.

### Incorrect aggregation
Mean vs median vs sum confusion, or grouping by the wrong key.

**Fix:** Check aggregated values against a hand calculation on a small subset.

### Preprocessing not recorded
The raw data was transformed (normalized, log-scaled, outliers removed) but this isn't documented.

**Fix:** Every transformation should be in the script, not done manually beforehand.

### Misleading axis scales
Log scales that hide important variation, or truncated axes that exaggerate differences.

**Fix:** Always start with linear scales and full ranges. Adjust only with justification.

## A prompt for figure generation

```text
Create a script to generate Figure 2 from my analysis.

Context:
- Data is in data/processed/results.csv
- Columns are: [list your columns]
- This figure shows [describe what it should show]

Requirements:
- Read parameters from a config file (configs/fig_2.yaml)
- Print data shape and summary stats before plotting
- Save figure to results/figures/fig_2.png
- Save provenance info to results/figures/fig_2_provenance.json
- Use a fixed random seed if any randomness is involved

Verification:
- Show me the first 5 rows of data being plotted
- Print min/max/mean of the plotted values
- The script should work from a fresh clone with `python scripts/make_fig_2.py`
```

## Tables follow the same rules

Tables in papers should be:

- Generated by a script, not manually typed
- Reproducible from raw data
- Include the same provenance tracking

For LaTeX tables, generate the `.tex` file programmatically:

```python
df.to_latex('results/tables/table_1.tex', index=False, float_format='%.3f')
```

## The "figure audit" before submission

Before submitting a paper, verify every figure:

1. Can you regenerate it from `make figures`?
2. Does the script show its data sources clearly?
3. Is every preprocessing step in the code (not done manually)?
4. Do the axis labels and legends match the actual data?
5. Is there a provenance record?

If any answer is "no," fix it before submission.

## See also

- [Data Analysis](data-analysis.md)
- [Reproducibility](reproducibility.md)
- [Prompt Templates](prompts.md)
