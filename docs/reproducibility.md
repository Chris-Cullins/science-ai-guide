# Reproducibility Habits (AI-Assisted Work)

Agentic AI can change many files quickly. Reproducibility is how you keep that power from becoming chaos.

This matters more, not less, when AI is involved. If you can't reproduce your results, you can't trust them—and neither can reviewers.

## The minimum bar

**"Fresh clone → one command → same key outputs."**

This is the reproducibility test. Someone (including future you) should be able to:

1. Clone your repository
2. Install dependencies
3. Run one command
4. Get the same figures/tables/results

If that works, you're in good shape. If it doesn't, you have a reproducibility problem.

## What to capture

### Inputs

- **Source data**: Where did it come from? What version? When was it accessed?
- **Preprocessing**: What transformations were applied before analysis?
- **External resources**: Any APIs, databases, or services used?

```markdown
# data/README.md

## raw/measurements.csv
- Source: Lab instrument export, 2025-11-15
- Instrument: Shimadzu LC-2050
- Operator: Jane Doe
- Notes: Contains raw peak areas, some saturated readings in column F

## processed/cleaned_data.csv
- Generated by: scripts/preprocess.py
- From: raw/measurements.csv
- Transformations: Remove saturated readings, normalize by internal standard
```

### Parameters

Don't bury parameters in code. Make them explicit:

```yaml
# configs/paper_figure1.yaml
data:
  input_path: data/processed/cleaned_data.csv
  test_fraction: 0.2

model:
  learning_rate: 0.001
  batch_size: 32
  epochs: 100

output:
  seed: 42
  figures_dir: results/figures/
```

Then your script loads the config:

```python
python scripts/make_figure.py --config configs/paper_figure1.yaml
```

Anyone can see exactly what parameters produced your results.

### Environment

Pin your dependencies. "It worked on my machine" is not reproducibility.

```bash
# Create requirements file
pip freeze > requirements.txt

# Or better, use a lockfile
pip-compile requirements.in > requirements.txt
```

For conda:
```bash
conda env export > environment.yml
```

Document OS assumptions if they matter (they often do for numerical code).

### Randomness

Many analyses involve randomness. Control it:

```python
import numpy as np
import random

# Set seeds at the start of your script
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# For PyTorch
import torch
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
```

Put the seed in your config file so it's documented and changeable.

### Outputs

Results should go to predictable locations:

```
results/
├── figures/
│   ├── figure1_main_result.pdf
│   └── figure2_comparison.pdf
├── tables/
│   └── table1_summary_stats.csv
└── logs/
    └── run_2025-11-15_14-32.log
```

## The two essential patterns

### Pattern 1: Makefile as source of truth

A Makefile (or `justfile`) encodes the canonical commands:

```makefile
.PHONY: install test figures clean all

install:
    pip install -r requirements.txt

test:
    pytest tests/ -v

figures: data/processed/cleaned_data.csv
    python scripts/make_figures.py --config configs/paper.yaml

data/processed/cleaned_data.csv: data/raw/measurements.csv
    python scripts/preprocess.py

clean:
    rm -rf results/figures/*
    rm -rf results/tables/*

all: install test figures
```

Now anyone can run `make figures` and get your results.

### Pattern 2: Configs folder with "paper-ready" configs

```
configs/
├── default.yaml           # Development defaults
├── paper_figure1.yaml     # Exact config for Figure 1
├── paper_figure2.yaml     # Exact config for Figure 2
└── supplementary.yaml     # Supplementary material
```

Check these into git. They document exactly what produced each result.

## AI-specific reproducibility concerns

### The agent changed many things

When an agent makes changes across multiple files, reproducibility gets harder. Mitigations:

1. **Commit frequently**: Small commits let you trace what changed when
2. **Review diffs carefully**: The agent might change things you didn't expect
3. **Run verification after each change**: Don't let errors accumulate

### Model versions matter

The same prompt can give different results with different model versions. Document which model you used:

```markdown
## Methods

Analysis code was developed with assistance from Claude Code (Claude Opus 4.5,
accessed November 2025). All code was reviewed and verified by the authors.
```

### Non-deterministic AI outputs

AI suggestions aren't deterministic. The same prompt might give different code on different days. This is fine if:

- You verify the output works
- You commit the actual code (not the prompt)
- The code itself is deterministic

## Testing reproducibility

Before you publish or share:

```bash
# Clone fresh copy
git clone your-repo /tmp/fresh-clone
cd /tmp/fresh-clone

# Follow your own setup instructions
pip install -r requirements.txt

# Run the canonical command
make figures

# Compare outputs
diff -r results/figures ../original-repo/results/figures
```

If this works, you're reproducible. If not, fix it now.

## Common reproducibility failures

### "It works on my machine"

- Missing dependencies not in requirements.txt
- Hardcoded absolute paths
- Relying on environment variables not documented

### "It worked last month"

- Dependencies updated and broke things
- Data source changed
- External API behavior changed

### "The figures look slightly different"

- Random seeds not set
- Floating point non-determinism (especially on GPU)
- Font/rendering differences (usually okay)

## A prompt for the agent

```text
Help me make this project reproducible.

Requirements:
- Add a Makefile with `make install`, `make test`, and `make figures`
- Pin all dependencies in requirements.txt
- Move hardcoded parameters to configs/default.yaml
- Add a data/README.md documenting data sources
- Set random seeds in all scripts that use randomness

Verification:
- Clone to a fresh directory
- Run make all
- Confirm outputs match
```

## See also

- [Project Structure](project-structure.md)
- [Data Analysis](data-analysis.md)
- [Figures and Tables](figures.md)
- [Verification and Rigor](verification.md)
- [Lab Rollout](lab-rollout.md)
